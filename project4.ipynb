{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, to_timestamp, dayofweek\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "config['CREDENTIAL']['AWS_ACCESS_KEY_ID'] = 'AKIA5NERC5RN3JOPBNL4'\n",
    "config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY'] = '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'\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['CREDENTIAL']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    ".config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "\n",
    "# read song data file\n",
    "df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOLLALT12A8C1399F3|Piano Concerto No...|ARWMEJW11F4C83C123|   0|319.37261|\n",
      "|SOAGZUH12A6D4FB4C5|The Sparrows And ...|AR4YEJU1187B991468|1991|191.26812|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract columns to create songs table\n",
    "songs_table = df.select(\n",
    "'song_id',\n",
    "'title',\n",
    "'artist_id',\n",
    "'year',\n",
    "'duration'\n",
    ").dropDuplicates()\n",
    "\n",
    "songs_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table\\\n",
    ".write\\\n",
    ".format('parquet')\\\n",
    ".mode('overwrite')\\\n",
    ".partitionBy('year', 'artist_id')\\\n",
    ".save(os.path.join(output_data, 'songs/songs.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table = df.select(\n",
    "'artist_id',\n",
    "col('artist_name').alias('name'),\n",
    "col('artist_location').alias(\"location\"),\n",
    "col('artist_latitude').alias('latitude'),\n",
    "col('artist_logitude').alias('longitude')\n",
    ").dropDuplicates\n",
    "\n",
    "artists_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table\\\n",
    ".write\\\n",
    ".format('parquet')\\\n",
    ".mode('overwrite')\\\n",
    ".save(os.path.join(output_data, 'artists/artists.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = input_data + \"log-data/*/*/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df = spark\\\n",
    "    .read\\\n",
    "    .format('json')\\\n",
    "    .load(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.where(col('page') == 'NextSong')\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table    \n",
    "users_table = df.select(\n",
    "col('userId').alias('user_id'),\n",
    "col('firstName').alias('first_name'),\n",
    "col('lastName').alias('last_name'),\n",
    "col('gender'),\n",
    "col('level')\n",
    ").dropDuplicates()\n",
    "\n",
    "users_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table\\\n",
    ".write\\\n",
    ".format('parquet')\\\n",
    ".mode('overwrite')\\\n",
    ".save(os.path.join(output_data, 'users/users.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_timestamp = udf(lambda x: str(int(int(x)/1000)))\n",
    "df = df.withColumn(\n",
    "    'ts',get_timestamp('ts')\n",
    ").withColumn('ts', to_timestamp(col('ts').cast(IntegerType())).alias('ts') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = df.select(col('ts').alias('start_time'))\\\n",
    ".withColumn('hour', hour(col('start_time')))\\\n",
    ".withColumn('day',dayofmonth(col('start_time')))\\\n",
    ".withColumn('week', weekofyear(col('start_time')))\\\n",
    ".withColumn('month',month(col('start_time')))\\\n",
    ".withColumn('year',year(col('start_time')))\\\n",
    ".withColumn('weekday',dayofweek(col('start_time')))\\\n",
    ".dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table\\\n",
    ".write\\\n",
    ".format('parquet')\\\n",
    ".mode('overwrite')\\\n",
    ".partitionBy('year','month')\\\n",
    ".save(os.path.join(output_data, 'time_table/time_table.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.json(input_data + 'song_data/*/*/*/*.json')\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.createOrReplaceTempView('songs_table')\n",
    "df.createOrReplaceTempView('log_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "joinExpr1 = df['artist'] == song_df['artist_name']\n",
    "joinExpr2 = df['length'] == song_df['duration']\n",
    "joinExpr3 = df['song'] == song_df['title']\n",
    "songplays_table= df.join(\n",
    "    song_df, joinExpr1 & joinExpr2 & joinExpr3, 'left_outer')\\\n",
    "    .where(col('userId').isNotNull())\\\n",
    "    .where(col('song_id').isNotNull())\\\n",
    "    .where(col('page') == 'NextSong')\\\n",
    "    .select(\n",
    "        col('ts').alias('start_time'),\n",
    "        col('userId').alias('user_id'),\n",
    "        col('level'),\n",
    "        col('song_id'),\n",
    "        col('artist_id'),\n",
    "        col('sessionId').alias('session_id'),\n",
    "        col('location'),\n",
    "        col('userAgent').alias('user_agent'),\n",
    "        year(col('ts')).alias('year'),\n",
    "        month(col('ts')).alias('month'))\\\n",
    "    .withColumn('songplay_id', monotonically_increasing_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table\\\n",
    "          .write\\\n",
    "          .format('parquet')\\\n",
    "          .mode('overwrite')\\\n",
    "          .partitionBy('year','month')\\\n",
    "          .save(os.path.join(output_data, 'songplays/songplays.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
